---
title: "Preliminary Analysis - Multiple Regression Approach"
author: "Willliam Ofosu Agyapong"
date: "4/19/2022"
output: pdf_document
---

```{r setup, include=FALSE}
# Set global options for output rendering
knitr::opts_chunk$set(echo = F, eval= T, warning = F, message = F, fig.align = "center")

# Set working directory to file path
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# Set default rounding to 4 decimal places
options(digits = 4)

# Import self-defined custom functions
source("functions.R")
```


```{r "Load libraries", echo=FALSE, eval=T, message=FALSE, warning=FALSE}
# Load required packages
library(summarytools)
library(dplyr)
library(tidyr)
library(purrr)
library(broom)
library(scales)
library(RColorBrewer)
library(ggplot2)
library(knitr)
library(kableExtra)
library(lmtest) # for the BP test

# Set default ggplot theme
theme_set(theme_classic())


# Load preprocessed data from phase I
# load("merged_TX_sub.RData")
```


```{r "Load data", eval=T}
covid <- read.csv("../covid.csv") # impact and hospitalization data
vul <- read.csv("../vulnerability.csv") # community vulnerability data
```


```{r "Munge data", eval=T}
# remove redundant X column
covid <- covid[, -1] 

# subset to Texas
vul_TX <- vul %>% filter(STATE=="TEXAS")

# merge by fips=hospital location
merged_TX <- inner_join(covid, vul_TX, by = c("fips_code"="County.FIPS"))

# take a look at summary stats
# merged_TX %>% dfSummary() %>% view()

# subset variables of interest and rename variables where necessary
merged_TX_sub <- merged_TX %>%
  select(inpatient_beds_7_day_avg, 
         inpatient_beds_used_covid_7_day_avg,
         total_icu_beds_7_day_avg,
         LIA_CS_PP = Low.Income.Area..LIA..County.SAIPE....Poverty.Percentage., 
         LIA_CT_PP = Low.Income.Area..LIA..Census.Tract..Poverty.Percentage.,
         is_metro_micro, HHA_Score = HHA.Score,
         Tribal_Community = Tribal.Community..1.if.yes., 
         Rural_Score = Rural...Score,
         hospital_subtype, city
         )

# Declare variable types
merged_TX_sub <- merged_TX_sub %>%
  mutate(
         # convert all character or integer variables to factors
         across(where(is.character) | where(is.integer), as.factor)
         )

# save(merged_TX_sub, file = "merged_TX_sub.RData")

```


```{r "Missing-Trt", eval=T}
# Treating missing values
merged_TX_sub[,1:3][merged_TX_sub[,1:3]==0] <- NA # Count zeros as missing values.
merged_TX_sub<- merged_TX_sub %>%na.omit() # Remove NAs

## remove other dependent variables
mod_df <- merged_TX_sub %>% select(-c(inpatient_beds_used_covid_7_day_avg,
                                      total_icu_beds_7_day_avg
                                      ))
```

## Another set of variables with more intuitive understanding
```{r}

merged_TX_sub2 <- merged_TX %>%
  select(inpatient_beds_7_day_avg, inpatient_beds_used_covid_7_day_avg, total_icu_beds_7_day_avg,
         is_metro_micro, HHA = Hardest.Hit.Area..HHA.,
         LIA_CS_PP = Low.Income.Area..LIA..County.SAIPE....Poverty.Percentage., 
         LIA_CT_PP = Low.Income.Area..LIA..Census.Tract..Poverty.Percentage.,
         Tribal_Community = Tribal.Community..1.if.yes., 
         Rural = Rural,
         hospital_subtype, city, zip
         )
merged_TX_sub2 <- merged_TX_sub2 %>%
  mutate(
         # convert all character or integer variables to factors
         across(where(is.character) | where(is.integer), as.factor)
         )
merged_TX_sub2[,1:3][merged_TX_sub2[,1:3]==0] <- NA # Count zeros as missing values.
merged_TX_sub2 <- merged_TX_sub2 %>%na.omit() # Remove NAs
```

# EDA

```{r}
# determine unique levels of categorical variables
length(unique(merged_TX_sub$hospital_subtype)) # 4
length(unique(merged_TX_sub$city)) # 116
length(unique(merged_TX_sub$zip)) # 170
length(unique(merged_TX_sub$COUNTY))

# Assess correlation
head(merged_TX_sub)
continuous_vars <- merged_TX_sub[, 1:3]
pairs(continuous_vars)
cor(continuous_vars) # LIA_CS_PP is highly correlated with the responses, but LIA_CS_PP and LIA_CT_PP

head(mod_df)
continuous_vars <- mod_df[, 1:3]
pairs(continuous_vars)
# are not strongly correlated less evidence of multicolinearity.

# prepare data for modeling: subset data to only variables to be used in the modeling
# data_for_mdl <- 
```



```{r}
full_mod <- lm(inpatient_beds_7_day_avg ~ . -inpatient_beds_used_covid_7_day_avg -total_icu_beds_7_day_avg -zip -city, data = merged_TX_sub)
summary(full_mod)

# check VIF
vif(full_mod)


full_mod2 <- lm(inpatient_beds_used_covid_7_day_avg ~ . -inpatient_beds_7_day_avg -total_icu_beds_7_day_avg - zip, data = merged_TX_sub)
summary(full_mod2)

full_mod3 <- lm(total_icu_beds_7_day_avg ~ . -inpatient_beds_7_day_avg -inpatient_beds_used_covid_7_day_avg, data = merged_TX_sub)
summary(full_mod3)
```

**inpatient_beds_7_day_avg**: Residual standard error = 274, R-squared = 0.329, adjusted r-squared = 0.327, f-stat is 184 with p-value < 0.001 (stronger evidence than the other two response variables).  Most levels of city not that significant. Individually, it is only the hospital subtype that appears significant. Overall, the f-test suggests that the model including all the predictors is better than using the mean of the response.

**inpatient_beds_used_covid_7_day_avg**: Similar performance for individual predictors but slight changes in the following measures. Residual standard error = 69, R-squared = 0.251, adjusted r-squared = 0.249, f-stat is 125 with p-value < 0.001.

**total_icu_beds_7_day_avg**: Similar performance for individual predictors but slight changes in the following measures. R-squared = 0.297, adjusted r-squared = 0.295, f-stat is 158 with p-value < 0.001.

Note: 
- Using the raw hardest hit and rural variables did not lead to any changes in performance, so I think it will be helpful to report those instead for easy interpretation of results.

- *Including zip code leads to a high proportion of variance explained of about 83% and much precision (RSE of about 137) for inpatient_beds_7_day_avg, but most of the coefficients are NAs.*

- The county variables does not appear to be a useful predictor.

## remove the LIA_CT_PP variable
```{r}
# remove the LIA_CT_PP variable
mod1 <- lm(inpatient_beds_7_day_avg ~ . -inpatient_beds_used_covid_7_day_avg -total_icu_beds_7_day_avg -zip -LIA_CT_PP, data = merged_TX_sub )
summary(mod1)

mod2 <- lm(inpatient_beds_used_covid_7_day_avg ~ . -inpatient_beds_7_day_avg -total_icu_beds_7_day_avg -zip -LIA_CT_PP, data = merged_TX_sub )
summary(mod2)

mod3 <- lm(total_icu_beds_7_day_avg ~ . -inpatient_beds_7_day_avg -inpatient_beds_used_covid_7_day_avg -zip -LIA_CT_PP, data = merged_TX_sub )
summary(mod3)
```

LIA_CT_PP is not a useful predictor, its removal causes no changes to the previous performances.

```{r}
# remove the city  and zip variable
mod1 <- lm(inpatient_beds_7_day_avg ~ . -inpatient_beds_used_covid_7_day_avg -total_icu_beds_7_day_avg -city, data = merged_TX_sub )
summary(mod1)

mod2 <- lm(inpatient_beds_used_covid_7_day_avg ~ . -inpatient_beds_7_day_avg -total_icu_beds_7_day_avg -zip -city, data = merged_TX_sub )
summary(mod2)

mod3 <- lm(total_icu_beds_7_day_avg ~ . -inpatient_beds_7_day_avg -inpatient_beds_used_covid_7_day_avg -zip -city, data = merged_TX_sub )
summary(mod3)
```

**inpatient_beds_7_day_avg**: R-squared = 0.0625, adjusted r-squared = 0.0623, f-stat is 278 with p-value < 0.001 (stronger evidence than the other two response variables).  Without city in the model, the performance of the model declined substantially in terms of variation explained, while most of the remaining predictors now show much significance. Overall, the f-test suggests that the model including all the predictors is better than using the sample mean to model the response.

**inpatient_beds_used_covid_7_day_avg**: Similar performance for individual predictors but slight improvement in the following measures. R-squared = 0.0699, adjusted r-squared = 0.0697, f-stat is 313 with p-value < 0.001.

**total_icu_beds_7_day_avg**: Similar performance for individual predictors but slight decline in the following measures. R-squared = 0.0538, adjusted r-squared = 0.0536, f-stat is 237 with p-value < 0.001.

# Automatic search

## Stepwise regression
```{r}
full_mod <- lm(inpatient_beds_7_day_avg ~ LIA_CS_PP + LIA_CT_PP + is_metro_micro + 
    HHA_Score + Tribal_Community + Rural_Score + hospital_subtype + 
    city, data = train.data)
summ(full_mod)

full_mod2 <- lm(inpatient_beds_used_covid_7_day_avg ~ LIA_CS_PP + LIA_CT_PP + is_metro_micro + 
    HHA_Score + Tribal_Community + Rural_Score + hospital_subtype + 
    city, data = train.data)
summ(full_mod2)

# rm(full_mod_)


full_mod3 <- lm(total_icu_beds_7_day_avg ~ . -inpatient_beds_7_day_avg -inpatient_beds_used_covid_7_day_avg, data = merged_TX_sub)

step(full_mod, direction = "forward") # drop none

step(full_mod, direction = "backward")
step(full_mod, direction = "both") # same result as backward but takes much time
final_mod <- lm(inpatient_beds_7_day_avg ~ hospital_subtype + zip, 
    data = merged_TX_sub) # LIA_CS_PP
summary(final_mod)

step(full_mod2, direction = "backward")
final_mod2 <- lm(formula = inpatient_beds_7_day_avg ~ hospital_subtype + 
    city, data = train.data) # including zip is better
summary(final_mod2)

BIC <- log(nrow(model.frame(full_mod)))
step(full_mod, dir = "backward", k = BIC)

step(full_mod3, direction = "backward")
final_mod3 <- lm(formula = total_icu_beds_7_day_avg ~ hospital_subtype + city + 
    zip, data = merged_TX_sub)
summary(final_mod3)
```

Without zip code: The best model is `inpatient_beds_7_day_avg ~ hospital_subtype + city`. Surprisingly, this does not include any of the community vulnerability measures of interest, so we resort to the second best model below which retained `LIA_CS_PP`.
`inpatient_beds_7_day_avg ~ LIA_CS_PP + hospital_subtype + city`. 

with zip code: `inpatient_beds_7_day_avg ~ hospital_subtype + city + zip`.

NB: Modeling the other dependent variables as responses also ended up with the same "best" subset of variables. However, the  inpatient_beds_7_day_avg response resulted in the best performance. Hence, we settle on this variable as our response of interest.


# Best subset
```{r}
# install.packages("bestglm")
# library(bestglm)
# bestglm(model.frame(fit)[c(2:6, 1)], IC = "AIC")$Subsets 
# model.frame(full_mod)

library(caret)
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(inpatient_beds_7_day_avg ~.-city, data = mod_df,
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:10),
                    trControl = train.control
                    )
step.model$results

summary(step.model$finalModel)

## stepwise with AIC
step.model <- train(inpatient_beds_7_day_avg ~., data = mod_df,
                    method = "lmStepAIC", 
                    # tuneGrid = data.frame(nvmax = 1:10),
                    trControl = train.control,
                    trace = FALSE
                    )
# Model accuracy
step.model$results
# Final model coefficients
step.model$finalModel
# Summary of the model
summary(step.model$finalModel)
```




# A further search for best model

Here, we want to build on the model selected by the backward selection automatic search procedure to see if any other combination of variables including interactions can result in improved performance and enhance model interpretability to help us achieve our goal of getting a model that provides both explonatory and predictive power.

- `hospital_subtype*HHA_Score + city` good without `LIA_CS_PP` but only leads to small reduction in F-statistic (from 217 - 215)

- `LIA_CS_PP*hospital_subtype+ hospital_subtype*HHA_Score + city`: including interaction term between LIA_CS_PP and hospital_subtype only increases R^2 by 0.01.

- `LIA_CS_PP*hospital_subtype+ hospital_subtype*HHA_Score + city*hospital_subtype`: increased R^2 to 0.40 and 

```{r "further search"}
# install.packages("jtools")
library(jtools)
# Consider interactions. hospital_subtype*HHA_Score appears to help
mod <- lm(inpatient_beds_7_day_avg ~ LIA_CS_PP + hospital_subtype*HHA_Score + city, 
    data = merged_TX_sub)
summ(mod)

mod <- lm(inpatient_beds_7_day_avg ~ LIA_CS_PP + hospital_subtype*HHA_Score + city, 
    data = merged_TX_sub)


# interactions with rural doesn't seem to help 
mod1 <- lm(inpatient_beds_7_day_avg ~ LIA_CS_PP*Rural_Score + hospital_subtype*Rural_Score + hospital_subtype*HHA_Score + city, 
    data = merged_TX_sub) # 
summ(mod1) # performs as good as with Rural_Score

# interactions with Tribal_Community doesn't seem to help 
mod1 <- lm(inpatient_beds_7_day_avg ~ LIA_CS_PP*Tribal_Community + hospital_subtype*Rural_Score + hospital_subtype*HHA_Score + city, 
    data = merged_TX_sub) # 
summ(mod1) # performs as good as with Rural_Score
```


```{r "interaction plots"}
install.packages("interactions")
library(interactions)
interact_plot(mod, pred=LIA_CS_PP, modx = hospital_subtype, plot.points = T, linearity.check = T) # the relationship is not linear

# fit a model with a polynomial term
selected_mod <- lm(inpatient_beds_7_day_avg ~ LIA_CS_PP + hospital_subtype*HHA_Score + city, 
    data = merged_TX_sub)

summ(selected_mod)

# diagnostics
plot(selected_mod)

mod <- lm(log(inpatient_beds_7_day_avg) ~ poly(LIA_CS_PP,2)+ hospital_subtype*HHA_Score + city, 
    data = merged_TX_sub) # R^2 = 0.51
summ(mod)


moda <- lm(log(inpatient_beds_7_day_avg) ~ log(LIA_CS_PP) + hospital_subtype * HHA_Score, 
    data = merged_TX_sub) # R^2 = 0.51
plot(moda)
summ(moda)
interact_plot(moda, pred = hospital_subtype, modx = HHA_Score)
cat_plot(moda, pred = hospital_subtype, modx = HHA_Score)

modb <- lm(log(inpatient_beds_used_covid_7_day_avg) ~ log(LIA_CS_PP) + hospital_subtype+HHA_Score +city, 
    data = merged_TX_sub)
modb2 <- lm(log(inpatient_beds_used_covid_7_day_avg) ~ poly(LIA_CS_PP,2) + hospital_subtype+HHA_Score +city, 
    data = merged_TX_sub)
summ(moda)
summ(modb2)
plot(modb)
plot(modb2)

mod <- lm(log(inpatient_beds_7_day_avg) ~ log(LIA_CS_PP) + hospital_subtype*HHA_Score + city, 
    data = merged_TX_sub, class = "validation") # R^2 = 0.51
summ(mod)
```



## Consider penalized regression models

`glmnet(x, y, alpha = 1, lambda = NULL)`

x: matrix of predictor variables
y: the response or outcome variable, which is a binary variable.
alpha: the elasticnet mixing parameter. Allowed values include:
“1”: for lasso regression
“0”: for ridge regression
a value between 0 and 1 (say 0.3) for elastic net regression.
lamba: a numeric value defining the amount of shrinkage. Should be specify by analyst.
In penalized regression, you need to specify a constant lambda to adjust the amount of the coefficient shrinkage. The best lambda for your data, can be defined as the lambda that minimize the cross-validation prediction error rate. This can be determined automatically using the function cv.glmnet().

```{r}
library(caret)
library(glmnet)
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
mod_df <- merged_TX_sub
training.samples <- mod_df$inpatient_beds_7_day_avg %>%
  createDataPartition(p = 0.7, list = FALSE)
train.data  <- mod_df[training.samples, ]
test.data <- mod_df[-training.samples, ]

# Predictor variables
X <- model.matrix(inpatient_beds_7_day_avg~., train.data)[,-1]
# Outcome variable
y <- train.data$inpatient_beds_7_day_avg

# Find the best lambda using cross-validation
set.seed(123) 
cv <- cv.glmnet(X, y, alpha = 0)
# Display the best lambda value
cv$lambda.min

# Fit the final model on the training data
model <- glmnet(X, y, alpha = 1, lambda = cv$lambda.min)
# Display regression coefficients
coef(model)

# Make predictions on the test data
x.test <- model.matrix(inpatient_beds_7_day_avg ~., test.data)[,-1]
predictions <- model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, test.data$inpatient_beds_7_day_avg),
  Rsquare = R2(predictions, test.data$inpatient_beds_7_day_avg)
)

# fit a LASSO model: good for variable selection
lasso <- train( inpatient_beds_7_day_avg ~ ., data = mod_df,
                method="glmnet", 
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale"),
                expand.grid(.alpha=1, .lambda=seq(0,0.1,0.01)))

# Fit a Ridge regression model
ridge <- fit.model("glmnet", expand.grid(.alpha=0, .lambda=seq(0,0.1,0.01)))

# mdl = lm(log(Petal.Length) ~ Species, data = iris)
# summ(mdl)
# plot(mdl)
# cbind(model.frame(mdl), resid=resid(mdl), fit=fitted(mdl)) %>%
#  ggplot(aes(fit, resid))  +
#   geom_point() +
#   facet_wrap(.~Species, )

plot(model)

ggplot(iris, aes(Species, Petal.Length)) + geom_point()
```

**Remarks**
- The city variable was later dropped from the model because the model without it show improvements in the assumption of normality and equal variances.
- We reasoned that the many levels of the city variable were probably causing overfitting.

- Surprisingly, a thorough investigation revealed that whenever city was included in the model, all community vulnerability measures were rendered insignificant such that our automatic selection procedure always dropped all of them from the model, leaving only hospital subtype. However, dropping city resulted in a drop in performance in terms of R^2 and the residual standard error.

## Suggestions for future work:

- Perform the automatic model selection with transformed variables.


```{r "model building", eval=F}

# Stepwise regression for automatic search
full_mod <- lm(inpatient_beds_7_day_avg ~ LIA_CS_PP + LIA_CT_PP + is_metro_micro + 
    HHA_Score + Tribal_Community + Rural_Score + hospital_subtype, data = train.data)
summary(full_mod) # 33% explained variability

step(full_mod, direction = "backward")
step(full_mod, direction = "both") # same result as backward but takes much time
final_mod <- lm(inpatient_beds_7_day_avg ~ hospital_subtype + city, 
    data = train.data) # LIA_CS_PP
summary(final_mod) # 33% explained variability

## Manual search
# ---- try transformations
final_mod2 <- lm(log(inpatient_beds_7_day_avg) ~ sqrt(LIA_CS_PP) +  hospital_subtype, data = train.data) # without city normality looks good, nonconstancy also looks quite good
summary(final_mod2)
plot(final_mod2) # sqrt(LIA_CS_PP) +

final_mod3 <- lm(log(inpatient_beds_7_day_avg) ~ log(LIA_CS_PP) +  hospital_subtype, data = train.data) # without city normality looks good, nonconstancy also looks quite good 
# this performs slightly better than the square root of LIA_CS_PP
summary(final_mod3)
plot(final_mod2)

final_mod4 <- lm(log(inpatient_beds_7_day_avg) ~ log(LIA_CS_PP) +  hospital_subtype*HHA_Score, data = train.data) # without city normality looks good, nonconstancy also looks quite good 
# this performs slightly better than the square root of LIA_CS_PP
summary(final_mod4)
plot(final_mod4)

final_mod5 <- lm(log(inpatient_beds_7_day_avg) ~ log(LIA_CS_PP) +  hospital_subtype*HHA_Score, data = train.data) # without city normality looks good, nonconstancy also looks quite good 
# this performs slightly better than the square root of LIA_CS_PP
summary(final_mod5)
plot(final_mod5)


```


```{r}
full_mod <- lm(inpatient_beds_7_day_avg ~ LIA_CS_PP + LIA_CT_PP + is_metro_micro + 
    HHA_Score + Tribal_Community + Rural_Score + hospital_subtype, data = train.data)
summary(full_mod) 
plot(full_mod)

full_mod <- lm(log(inpatient_beds_7_day_avg) ~ LIA_CS_PP + LIA_CT_PP + is_metro_micro + 
    HHA_Score + Tribal_Community + Rural_Score + hospital_subtype, data = train.data)
summary(full_mod) 
plot(full_mod)

full_mod <- lm(log(inpatient_beds_7_day_avg) ~ log(LIA_CS_PP) + LIA_CT_PP + is_metro_micro + 
    HHA_Score + Tribal_Community + Rural_Score + hospital_subtype, data = train.data)
summary(full_mod)  # improvement in R^2, RSE 
plot(full_mod)

full_mod <- lm(log(inpatient_beds_7_day_avg) ~ log(LIA_CS_PP) + log(LIA_CT_PP+1) + is_metro_micro + 
    HHA_Score + Tribal_Community + Rural_Score + hospital_subtype, data = train.data)
summary(full_mod)  # no improvement 
plot(full_mod)

full_mod <- lm(log(inpatient_beds_7_day_avg) ~ log(LIA_CS_PP) + is_metro_micro + 
    HHA_Score + Tribal_Community + Rural_Score + hospital_subtype, data = train.data)
summary(full_mod)  # no improvement 
plot(full_mod)
```



